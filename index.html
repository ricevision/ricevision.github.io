<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Computer Vision Group at Rice University</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://ricevision.github.io">Computer Vision @ Rice</a></h1>
        <p class="view"><img style="width:84%" src="logo.png" alt="logo of an owl with camera shutters for eyes"/></p>
        <p>Supported by Ken Kennedy Institute (Rice University)</p>
                
        Members: <br>
        <a href="http://www.cs.rice.edu/~vo9/">Vicente Ord&oacute;&ntilde;ez (Rice CS)</a> <br>
        <a href="https://www.guhabalakrishnan.com/">Guha Balakrishnan (Rice ECE)</a> <br>        
        <a href="https://vivekboominathan.com/">Vivek Boominathan (Rice ECE)</a> <br>
        <a href="https://computationalimaging.rice.edu/team/ashok-veeraraghavan/">Ashok Veeraraghavan (Rice ECE)</a> <br>
        <br>     
      </header>
      <section>
       
        <p> 
          Our group combines the expertise of multiple research labs at Rice University through the 
          <a href="https://kenkennedy.rice.edu/">Ken Kennedy Institute</a> to advance computer vision.
          We are especially interested in developing computer vision systems for dynamic and interactive settings where
          continuous interplay with the environment and adaptation to real-time changes are required.
          We envision reactive and adaptive computer vision systems, specifically through the implementation 
          of closed-loop systems that can take advantage of continuous feedback from
          complementary modalities such as sound, speech, temperature, and other environment variables. 
          We envision models that can work in a diverse array of image domains and dynamically adapt to environmental conditions on-the-fly 
          without human intervention
          in response to both sensor data and detected environment variable changes.
        </p>

      
          <h4> Publications </h4>
          <ul>
            <li class="media">
              <div class="media-body">
                <p class="my-auto">
                <a class="blue_link" href="https://arxiv.org/abs/2403.13199">
                  DecentNeRFs: Decentralized Neural Radiance Fields 
                </a> <br/>
                <span class="pub_authors d-lg-block">
                  Zaid Tasneem, Akshat Dave, Abhishek Singh, Kushagra Tiwary, Praneeth Vepakomma, Ashok Veeraraghavan, Ramesh Raskar
                </span>
                <span class="pub_info  d-lg-block">
                  European Conference on Computer Vision.<strong> ECCV 2024</strong>. Milan, Italy.
                </span>
                [<a href="https://zaidtas.github.io/decentnerfs/index.html">project page</a>]
                [<a href="https://arxiv.org/abs/2403.13199">arxiv</a>]
                </p>
              </div>
            </li>

            <li class="media">
              <div class="media-body">
                <p class="my-auto">
                <a class="blue_link" href="https://arxiv.org/abs/2303.12001v2">
                  ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders
                </a> <br/>
                <span class="pub_authors d-lg-block">
                  Jefferson Hernandez, Ruben Villegas, Vicente Ordonez. 
                </span>
                <span class="pub_info  d-lg-block">
                  European Conference on Computer Vision.<strong> ECCV 2024</strong>. Milan, Italy.
                </span>
                [<a href="https://jeffhernandez1995.github.io/vicmae/">project page</a>]
                [<a href="https://arxiv.org/abs/2303.12001">arxiv</a>]
                [<a href="https://github.com/jeffhernandez1995/ViC-MAE">github</a>]
                </p>
              </div>
            </li>
  
            <li class="media">
              <div class="media-body">
                <p class="my-auto">
                <a class="blue_link" href="https://arxiv.org/abs/2402.18695">
                  Grounding Language Models for Visual Entity Recognition 
                </a> <br/>
                <span class="pub_authors d-lg-block">
                  Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, Vicente Ordonez.
                </span>
                <span class="pub_info  d-lg-block">
                  European Conference on Computer Vision.<strong> ECCV 2024</strong>. Milan, Italy.
                </span>
                  [<a href="https://github.com/MrZilinXiao/AutoVER">github</a>] 
                  [<a href="https://arxiv.org/abs/2402.18695">arxiv</a>]
                </p>
              </div>
            </li>
          </ul>
        </p>
      </section>
      <footer>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
  </body>
</html>
